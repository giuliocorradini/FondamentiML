# Metodologie di learning

Il predittore deve essere generale, ovvero deve essere in grado di inferire uno schema che modelli i dati con un buon
grado di generalità. La $F$ deve "poter prevedere" anche su dati che non sono presenti nel training set originale.

Prendiamo l'esempio dalle slide con le due classi. Con funzioni non lineari posso stabilire una classificazione che
minimizza l'errore sul training set, tuttavia questo potrebbe portare ad errori molto grandi nel caso generale.
Quello che il modello deve evitare è proprio questo, l'errore in cui si è caduti si chiama _overfitting_.

[Overfitting and bias-variance](https://datascience.stackexchange.com/questions/45578/why-underfitting-is-called-high-bias-and-overfitting-is-called-high-variance)

## Setting matematico

I dati giacciono in $X$ spazio di input e sono etichettati con delle label che risiedono nello spazio di output $Y$.

Il training set è un insieme di coppie $(x_i, y_i)$ prodotto dello spazio di input e spazio di output. L'insieme D.

Le funzioni predittore, di cui fa parte la F che vogliamo trovare, che stabiliscono una relazione tra gli $x_i$ e gli
$y_i$, formano uno spazio vettoriale chiamato spazio delle ipotesi $\mathcal H$.

Un modello generico, che chiamiamo $f^*$, deve risolvere il problema di minimo:

$$
\min_ {f \in \mathcal H} \sum_{i=1}^{N} V(y_i, f(x_i)) + \lambda ||f||^2
$$

Il criterio per cui scegliere la formula è il termine della sommatoria.

Il termine $\sum_{i=1}^{N} V(y_i, f(x_i))$ è una misura dell'errore del predittore sul training set.
V è una funzione che misura l'errore sul training set ed è anche detta _rischio empirico_ in termini statistici.

Se ci fosse solo il termine blu avrei creato le condizioni per fare overfitting.
Aggiungendo invece il termine rosso $||f||^2$ è una norma di f che misura la complessità del modello.

Il moltipicatore $\lambda$ quantifica il tradeoff tra rischio empirico e complessità. Possiamo forzare funzioni a
bassa complessità per non overfittare. È detto _parametro di regolarizzazione_.

Quindi $f^*$ ottimo dipende dallo spazio delle ipotesi, dalla definizione della norma di f (complessità) e dalla
definizione di _loss function_ $V(y, f(x))$.

### SVM Setting

L'SVM ricade esattamente nello schema generale. Preso K funzione Kernel:

$$
f^*(x) = \sum_{i=1}^{N} c_i^* K(x, x_i) + b^*
$$
