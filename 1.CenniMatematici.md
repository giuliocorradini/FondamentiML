# Richiami

I versori canonici di $R^n$ sono n vettori del tipo:

$$
\hat e_k = (0, ..., 1, ..., 0)
$$

ovvero vettori che hanno 1 nella k-esima posizione e zero nelle altre. Il cappello indica che sono versori, ovvero la
loro norma è unitaria.

Vengono utilizzati per la cosiddetta _one-hot encoding_ delle label.

## Norma

La norma è un'operazione scalare $|•|: R^n \to R$ che introduce il concetto di distanza. Sui numeri reali abbiamo un
ordinamento indotto.

Ritorna nell'overfitting: voglio sì imaprare dal dataset (bias) ma mantenere una generalità per poter fare predizioni
su un dataset ignoto. Il problema è che se divento troppo bravo per fittare sugli elementi del trainset, vado a
overfittare. Ci sono degli elementi di regolarizzazione per evitare overfitting.

Il modulo definisce la distanza dall'origine, e in generale si può definire una distanza tra vettori:

$$
| v - w | = \sqrt{\sum_{i=0}^n (x_i - y_i)^2}
$$

Spesso chiamiamo distanza anche ciò che distanza non è: ci sono misure di similarità come la cosine similarity che
vengono chiamate distanze.

Tra le proprietà del modulo citiamo nuovamente la disuguaglianza triangolare, molto importante:

$$
| v_1 + v_2 | \le | v_1 | + | v_2 |
$$

Pensiamo invece alla cosine similarity, non vale più la disuagualianza triangolare, quindi è sbagliato definirla distanza.

## Prodotto scalare

Il prodotto scalare tra due vettori associa a una coppia di vettori v e w, uno scalare $<v, w>$ oppure $v \cdot w$:

$$
\cdot : R^n \times R^n \to R \\
\langle v, w \rangle = v \cdot w = \sum_{i=1}^n x_i y_i
$$

in inglese si chiama dot product. Si contrappone al prodotto element-wise che si indica con un punto cerchiato.

Proprietà:

$$
\langle v, w \rangle = |v| \cdot |w| \cdot \cos (\theta)
$$

Da qui deduciamo che presi due vettori non nulli, allora $\langle v, w \rangle = 0 \Leftrightarrow v, w \ \text{ortogonali}$.

Vale la disuguaglianza di Cauchy-Schwarz, cioè:

$$
\langle v, w \rangle \le |v| \cdot |w|
$$

Per completezza definiamo anche il prodotto vettoriale, che individua il vettore ortogonale all'iperpiano su cui giacciono i vettori.
Non è sempre possibile definirlo.

## Derivate

Partiamo generalizzando il concetto di derivata per $R^n$.

> Varietà affine: dato un vettore $s \in \mathbb R^n$, si dice **varietà affine** di $\mathbb R^n$ passante per $x_0 \in \mathbb R^n$ la traslazione
> del sottospazio generato da $s \in x_0$.
> $$
x(\alpha) = x_0 + \alpha s, \ \alpha \in \mathbb R
$$

Definiamo quindi la derivata direzionale di una funzione $f: \mathbb R^n \to \mathbb R$ rispetto alla direzione di s in
$x_0$ il seguente limite:

$$
d_sf = \lim_{\alpha \to 0^+} \frac{f(x_0 +\alpha s) - f(x_0)}{\alpha}
$$

Se tale limite esiste finito, la funzione f ha derivata direzionale rispetto a s.

La derivata direzionale rispetto ai versori canonici permette di ricavare le derivate direzionali.
Il vettore delle operazioni di derivate direzionali si chiama nabla e si indica come:

$$
\nabla = \begin{pmatrix} \delta \over \delta x_{1} \\ ... \\ \delta \over \delta x_{3} \end{pmatrix}
$$

moltplicando col prodotto vettoriale per una funzione otteniamo le sue derivate parziali.

Il vettore delle derivate parziali si chiama 

## Hessiano

Se $f \in \mathcal C ^ 2 (\mathbb R^n)$ allora è possibile differenziare ogni derivata parziale rispetto a ogni versore
canonico, otteniamo quindi la matrice hessiana, ovvero delle derivate parziali miste.

Dal teorema di Schwarz tale matrice è simmetrica. Ovvero se le derivate secondo miste sono continue, allora è ininfluente
l'ordine con cui si fanno le derivate parziali.

$$
\nabla^2 f = H =
\begin{bmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

In un contesto di SVM o regression $\mathbb R^n$ è lo spazio del dataset, mentre in deep learning è lo spazio dei parametri.

L'Hessiana viene utilizzati nei metodi c.d. del secondo ordine.

## Jacobiano

Matrice rettangolare $n \cdot m$, e si definisce come:

$$
\nabla f(x) = (\nabla f_1(x) \ \dots \ \nabla f_1(x))
$$

serve a fare back-propagation.

Regola pratica per il calcolo della derivata direzionale: TODO

Le funzioni quadratiche sono le funzioni quadratiche. Le usiamo perché sono funzioni di tipo loss e voglio individuare dei
minimi. Lo uso nell'ottimizzazione.

La variazione è il gradiente, l'hessiana è la curvatura. Analogamente in $\mathbb R$ la concavità è data dalla derivata seconda.

### Rosenbrock

Sia data la funzione $f : \mathbb R^2 \to \mathbb R$

$$
f(x) = 100(x_2 - x_1^2)^2 + (1-x_1)^2
$$
